{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, initializers\n",
    "from AtomasWrapper import AtomasWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple for now\n",
    "def create_q_model():\n",
    "\n",
    "    state = layers.Input(shape = (20, 1))\n",
    "\n",
    "    layer1 = layers.Conv1D(\n",
    "        filters = 256, \n",
    "        kernel_size = 20, \n",
    "        strides = 1, \n",
    "        activation = \"relu\",\n",
    "        kernel_initializer = initializers.TruncatedNormal(mean = 0., stddev = 0.01),\n",
    "        bias_initializer = initializers.Constant(0.01)\n",
    "    )(state)\n",
    "\n",
    "    layer2 = layers.Flatten()(layer1)\n",
    "\n",
    "    layer3 = layers.Dense(\n",
    "        units = 128, \n",
    "        activation = \"linear\",\n",
    "        kernel_initializer = initializers.TruncatedNormal(mean = 0., stddev = 0.01),\n",
    "        bias_initializer = initializers.Constant(0.01)\n",
    "    )(layer2)\n",
    "\n",
    "    layer4 = layers.Dense(\n",
    "        units = 128, \n",
    "        activation = \"linear\",\n",
    "        kernel_initializer = initializers.TruncatedNormal(mean = 0., stddev = 0.01),\n",
    "        bias_initializer = initializers.Constant(0.01)\n",
    "    )(layer3)\n",
    "\n",
    "    q_value = layers.Dense(\n",
    "        units = 19, \n",
    "        activation = \"linear\",\n",
    "        kernel_initializer = initializers.TruncatedNormal(mean=0., stddev=0.01),\n",
    "        bias_initializer=initializers.Constant(0.01)\n",
    "    )(layer4)\n",
    "\n",
    "    return keras.Model(inputs = state, outputs = q_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_q_model()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training procedure adapted from Yale's S&DS 365: Intermediate Machine Learning, Assignment 4.3.\n",
    "\n",
    "NUM_ACTIONS = 19\n",
    "\n",
    "GAMMA = 0.99            # decay rate of past observations\n",
    "STEP_SIZE = 1e-4        # step size\n",
    "OBSERVE = 10000         # timesteps to observe before training\n",
    "TRAINING = 10000000       # timesteps of observing + training\n",
    "REPLAY_MEMORY = 10000   # number of previous transitions to remember\n",
    "BATCH_SIZE = 32         # size of each batch\n",
    "EPSILON = 0.2           # exploration probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dql_atomas(model, optimizer, loss_function):\n",
    "\n",
    "    # initiate game\n",
    "    game = AtomasWrapper()\n",
    "\n",
    "    # store the previous state, action and transitions\n",
    "    history_data = deque()\n",
    "\n",
    "    # get the first observation by doing nothing and preprocess the image\n",
    "    current_state, reward, terminal = game.check()\n",
    "  \n",
    "    # training\n",
    "    t = 0\n",
    "\n",
    "    while t < TRAINING:\n",
    "\n",
    "        if np.random.rand(1)[0] < EPSILON:\n",
    "            # random action\n",
    "            action = np.random.choice(NUM_ACTIONS)\n",
    "        else:\n",
    "            # compute the Q function\n",
    "            current_state_tensor = tf.convert_to_tensor(current_state)\n",
    "            current_state_tensor = tf.expand_dims(current_state_tensor, 0)\n",
    "            q_value = model(current_state_tensor, training = False)\n",
    "          \n",
    "            # greedy action   \n",
    "            action = tf.argmax(q_value[0]).numpy()\n",
    "\n",
    "        # take the action and observe the reward and the next state\n",
    "        action_vec = np.zeros(NUM_ACTIONS)\n",
    "        action_vec[action] = 1\n",
    "        next_state, reward, terminal = game.step(action_vec)\n",
    "\n",
    "        # store the observation\n",
    "        history_data.append((current_state, action, reward, next_state, \n",
    "                            terminal))\n",
    "        if len(history_data) > REPLAY_MEMORY:\n",
    "            history_data.popleft()  # discard old data\n",
    "\n",
    "\n",
    "        # train if done observing\n",
    "        if t > OBSERVE:\n",
    "\n",
    "            # sample a batch\n",
    "            batch = random.sample(history_data, BATCH_SIZE)\n",
    "            state_sample = np.array([d[0] for d in batch])\n",
    "            action_sample = np.array([d[1] for d in batch])\n",
    "            reward_sample = np.array([d[2] for d in batch])\n",
    "            state_next_sample = np.array([d[3] for d in batch])\n",
    "            terminal_sample = np.array([d[4] for d in batch])\n",
    "\n",
    "            # compute the updated Q-values for the samples\n",
    "            future_rewards = model(tf.convert_to_tensor(state_next_sample), training = True)\n",
    "            terminal_tensor = tf.convert_to_tensor([float(s) for s in terminal_sample])   \n",
    "            updated_q_value = reward_sample + (GAMMA * tf.reduce_max(future_rewards, axis = 1))\n",
    "\n",
    "            # If final frame set the last value to -1\n",
    "            updated_q_value = updated_q_value * (1 - terminal_tensor) - (terminal_tensor * 16)\n",
    "\n",
    "            # train the model on the states and updated Q-values\n",
    "            with tf.GradientTape() as tape:\n",
    "\n",
    "                # compute the current Q-values for the samples\n",
    "                current_q_value = model(state_sample, training = True)\n",
    "                mask = tf.one_hot(action_sample, NUM_ACTIONS)\n",
    "                current_q_value = tf.reduce_sum(tf.multiply(current_q_value, mask), axis = 1)\n",
    "\n",
    "                # compute the loss\n",
    "                loss = loss_function(updated_q_value, current_q_value)\n",
    "\n",
    "            # backpropagation\n",
    "            grads = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        else:\n",
    "            loss = 0\n",
    "\n",
    "        # update current state and counter\n",
    "        current_state = next_state\n",
    "        t += 1\n",
    "\n",
    "        if t % 500 == 0: # originally 500\n",
    "            print(f\"STEP {t} | PHASE {'observe' if t <= OBSERVE else 'train'}\", \n",
    "                  f\"| ACTION {action} | REWARD {reward} | LOSS {loss}\")\n",
    "            if t and not t % 10000:\n",
    "                model.save(\"first_model\")\n",
    "                wrapper = AtomasWrapper()\n",
    "                wrapper.activate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_atomas(start_from_ckpt = False, ckpt_path = None):\n",
    "\n",
    "    np.random.seed(37)\n",
    "\n",
    "    if start_from_ckpt:\n",
    "        # if you want to start from a checkpoint\n",
    "        model = keras.models.load_model(\"first_model\")\n",
    "    else:\n",
    "        model = create_q_model()\n",
    "\n",
    "    # specify the optimizer and loss function\n",
    "    optimizer = keras.optimizers.Adam(learning_rate = STEP_SIZE, clipnorm = 1.0)\n",
    "    loss_function = keras.losses.MeanSquaredError()\n",
    "\n",
    "    # train model\n",
    "    dql_atomas(model = model, optimizer = optimizer, loss_function = loss_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_atomas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('atomas_venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "bac61cb5a93b6ed7e79ba85065ce9c69457c2467a0a26df70aa7fc6d30787d2e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
